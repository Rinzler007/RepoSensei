# Provider: "ollama" (default) or "openai" (optional)
LLM_PROVIDER=ollama

# Output mode: "strict" (default) or "helpful"
# strict: never invent routes/features; only state whatâ€™s evidenced
# helpful: can add "likely" suggestions, clearly labeled
RESPONSE_MODE=strict

# Ollama settings
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=qwen2.5:7b-instruct

# OpenAI settings (optional)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4.1-mini